{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-22T16:32:49.340597Z",
     "start_time": "2023-12-22T16:32:49.306681Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "            self.Wcf = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "            self.Wco = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(self.step):\n",
    "            x = input\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                if step == 0:\n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             shape=(height, width))\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            # only record effective steps\n",
    "            if step in self.effective_step:\n",
    "                outputs.append(x)\n",
    "\n",
    "        return outputs, (x, new_c)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0016895097436098\n",
      "Epoch [2/100], Loss: 1.0006560176339039\n",
      "Epoch [3/100], Loss: 0.9995398638365083\n",
      "Epoch [4/100], Loss: 0.9981229490558847\n",
      "Epoch [5/100], Loss: 0.9961686438159006\n",
      "Epoch [6/100], Loss: 0.9932281265933663\n",
      "Epoch [7/100], Loss: 0.988249847070243\n",
      "Epoch [8/100], Loss: 0.9806718405743011\n",
      "Epoch [9/100], Loss: 0.9678037742029018\n",
      "Epoch [10/100], Loss: 0.942053006647846\n",
      "Epoch [11/100], Loss: 0.9251712045825032\n",
      "Epoch [12/100], Loss: 0.8927080727878838\n",
      "Epoch [13/100], Loss: 0.8767168940144288\n",
      "Epoch [14/100], Loss: 0.8534190366007661\n",
      "Epoch [15/100], Loss: 0.8304248631768127\n",
      "Epoch [16/100], Loss: 0.8186437499748442\n",
      "Epoch [17/100], Loss: 0.7928802890547193\n",
      "Epoch [18/100], Loss: 0.7720165659785115\n",
      "Epoch [19/100], Loss: 0.7567803676172136\n",
      "Epoch [20/100], Loss: 0.7361922555798844\n",
      "Epoch [21/100], Loss: 0.7192944833019449\n",
      "Epoch [22/100], Loss: 0.7023807788098542\n",
      "Epoch [23/100], Loss: 0.6858882748021875\n",
      "Epoch [24/100], Loss: 0.6697321420889324\n",
      "Epoch [25/100], Loss: 0.6556084049683866\n",
      "Epoch [26/100], Loss: 0.6409220315924722\n",
      "Epoch [27/100], Loss: 0.6286698645855978\n",
      "Epoch [28/100], Loss: 0.6177519968119565\n",
      "Epoch [29/100], Loss: 0.606987242921041\n",
      "Epoch [30/100], Loss: 0.5924424703802327\n",
      "Epoch [31/100], Loss: 0.5826784843982162\n",
      "Epoch [32/100], Loss: 0.5716519217237652\n",
      "Epoch [33/100], Loss: 0.557445247062956\n",
      "Epoch [34/100], Loss: 0.5492703384810329\n",
      "Epoch [35/100], Loss: 0.5368769382986854\n",
      "Epoch [36/100], Loss: 0.5277628648858451\n",
      "Epoch [37/100], Loss: 0.5183489427662396\n",
      "Epoch [38/100], Loss: 0.5099815575582378\n",
      "Epoch [39/100], Loss: 0.5016893329259937\n",
      "Epoch [40/100], Loss: 0.4923574977257819\n",
      "Epoch [41/100], Loss: 0.48480200396810247\n",
      "Epoch [42/100], Loss: 0.4775843601239257\n",
      "Epoch [43/100], Loss: 0.470867740912931\n",
      "Epoch [44/100], Loss: 0.46579414472433556\n",
      "Epoch [45/100], Loss: 0.46773701517765787\n",
      "Epoch [46/100], Loss: 0.46702453596089655\n",
      "Epoch [47/100], Loss: 0.45259178207889394\n",
      "Epoch [48/100], Loss: 0.444817436346165\n",
      "Epoch [49/100], Loss: 0.43951360425620645\n",
      "Epoch [50/100], Loss: 0.4314661219763487\n",
      "Epoch [51/100], Loss: 0.42564646575256193\n",
      "Epoch [52/100], Loss: 0.4214582696465242\n",
      "Epoch [53/100], Loss: 0.4145082619789634\n",
      "Epoch [54/100], Loss: 0.41124187327498296\n",
      "Epoch [55/100], Loss: 0.4043074897612803\n",
      "Epoch [56/100], Loss: 0.39998725251143663\n",
      "Epoch [57/100], Loss: 0.39710955197351006\n",
      "Epoch [58/100], Loss: 0.3912392457166599\n",
      "Epoch [59/100], Loss: 0.387948233315229\n",
      "Epoch [60/100], Loss: 0.38176772080602384\n",
      "Epoch [61/100], Loss: 0.3794080916709678\n",
      "Epoch [62/100], Loss: 0.37646432971341826\n",
      "Epoch [63/100], Loss: 0.3784076126583812\n",
      "Epoch [64/100], Loss: 0.38985131943181167\n",
      "Epoch [65/100], Loss: 0.38724923035599246\n",
      "Epoch [66/100], Loss: 0.36765222935613967\n",
      "Epoch [67/100], Loss: 0.37310921542118525\n",
      "Epoch [68/100], Loss: 0.36149232956092203\n",
      "Epoch [69/100], Loss: 0.3606823964416122\n",
      "Epoch [70/100], Loss: 0.35776756881348737\n",
      "Epoch [71/100], Loss: 0.3515508487770803\n",
      "Epoch [72/100], Loss: 0.35076626698735736\n",
      "Epoch [73/100], Loss: 0.34694659840984643\n",
      "Epoch [74/100], Loss: 0.3427733846431278\n",
      "Epoch [75/100], Loss: 0.3409708190663624\n",
      "Epoch [76/100], Loss: 0.33837083217477426\n",
      "Epoch [77/100], Loss: 0.3346271683729203\n",
      "Epoch [78/100], Loss: 0.33315653149802155\n",
      "Epoch [79/100], Loss: 0.33067460342880994\n",
      "Epoch [80/100], Loss: 0.32905635342960565\n",
      "Epoch [81/100], Loss: 0.3302977235116432\n",
      "Epoch [82/100], Loss: 0.3321240603798382\n",
      "Epoch [83/100], Loss: 0.32821785892217936\n",
      "Epoch [84/100], Loss: 0.321873294599332\n",
      "Epoch [85/100], Loss: 0.32193858360349303\n",
      "Epoch [86/100], Loss: 0.31930209529928655\n",
      "Epoch [87/100], Loss: 0.31844262242635246\n",
      "Epoch [88/100], Loss: 0.3178852196470538\n",
      "Epoch [89/100], Loss: 0.314374213426037\n",
      "Epoch [90/100], Loss: 0.31426739456302405\n",
      "Epoch [91/100], Loss: 0.3080809094639121\n",
      "Epoch [92/100], Loss: 0.3089729246222018\n",
      "Epoch [93/100], Loss: 0.3067839810877152\n",
      "Epoch [94/100], Loss: 0.3050145404607858\n",
      "Epoch [95/100], Loss: 0.3015670084621995\n",
      "Epoch [96/100], Loss: 0.3009554264869961\n",
      "Epoch [97/100], Loss: 0.2976054898344873\n",
      "Epoch [98/100], Loss: 0.2965208850048383\n",
      "Epoch [99/100], Loss: 0.29495372736715936\n",
      "Epoch [100/100], Loss: 0.29347192914217557\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size=3, step=5,\n",
    "                    effective_step=[4])\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = Adam(convlstm.parameters(), lr=0.001)\n",
    "\n",
    "input = Variable(torch.randn(1, 512, 64, 32))\n",
    "target = Variable(torch.randn(1, 32, 64, 32)).double()\n",
    "\n",
    "# output = convlstm(input)\n",
    "# output = output[0][0].double()\n",
    "# res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "# print(res)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = convlstm(input)\n",
    "    output = output[0][0].double()\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T16:44:46.665848Z",
     "start_time": "2023-12-22T16:43:13.496048Z"
    }
   },
   "id": "5b69a530316182e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 32, 64, 32])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T16:57:19.464742Z",
     "start_time": "2023-12-22T16:57:19.457439Z"
    }
   },
   "id": "c55a14a3b08d6069",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e896ff37b837e45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
